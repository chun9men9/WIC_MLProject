header = TRUE,
na.strings = "",
stringsAsFactors = FALSE)
dim(house_train)
dim(house_test)
str(house_train)
## get levels of categorical features from data description
factorLevel <- list()
conn <- file("../data/data_description.txt", open="r")
f <-readLines(conn)
for (line in f){
if(!grepl("^[[:blank:]]", line) & grepl(": ", line)) {
col_name <<- trimws(gsub(":.*", "", line))
} else {
level <- trimws(gsub("\t.*", "", line))
if (level != "") {
factorLevel[[col_name]] <- c(factorLevel[[col_name]], level)
}
}
}
close(conn)
print(factorLevel[1:6])
## check if levels in description cover unique data values
for (varname in names(factorLevel)) {
levelDiff <- setdiff(unique(house_train[[varname]]),
factorLevel[[varname]])
if(length(levelDiff)) {
print(paste(varname,
paste(levelDiff, collapse = ", "),
sep = ": "))
}
}
## fix those levels that don't match with data
## ignore "NA" as they will be considered as missing when converting categorical to factors
unique(house_train$MSZoning)
factorLevel$MSZoning
factorLevel$MSZoning[2] <- "C (all)"
unique(house_train$Neighborhood)
factorLevel$Neighborhood
factorLevel$Neighborhood[13] <- "NAmes"
unique(house_train$BldgType)
factorLevel$BldgType
factorLevel$BldgType[c(2,3,5)] <- c("2fmCon","Duplex","Twnhs")
unique(house_train$Exterior2nd)
factorLevel$Exterior2nd
factorLevel$Exterior2nd[c(17,6,3)] <- c("Wd Shng","CmentBd","Brk Cmn")
## Get levels that only appear in the dataset
for (varname in names(factorLevel)) {
factorLevel[[varname]] <- intersect(factorLevel[[varname]],
unique(house_train[[varname]]))
}
## Re-run the previous cell to double check
## convert column datatype to numeric / factor
## On training dataset
for (varname in names(house_train)[-1]) {
if (varname %in% names(factorLevel)) {
house_train[[varname]] <- factor(house_train[[varname]],
levels = factorLevel[[varname]])
} else {
house_train[[varname]] <- as.numeric(house_train[[varname]])
}
}
## On testing dataset
for (varname in names(house_test)[-1]) {
if (varname %in% names(factorLevel)) {
house_test[[varname]] <- factor(house_test[[varname]],
levels = factorLevel[[varname]])
} else {
house_test[[varname]] <- as.numeric(house_test[[varname]])
}
}
house_train$Id <- NULL
rownames(house_test) <- house_test$Id
house_test$Id <- NULL
save(house_train, house_test, file = "./house_loaded.RData")
library(ggplot2)
library(gridExtra)
library(tabplot)
library(lsr)
library(corrplot)
install.packages('corrplot')
knitr::opts_chunk$set(echo = TRUE,
cache = TRUE,
warning = FALSE,
message = FALSE,
tidy=FALSE,
fig.height=6,
fig.width=10)
# setwd("~/Workspace/kaggle/housePriceAmes/")
setwd("C:/Users/cmlim/Dropbox/NYC Data Science/Class/Machine Learning Project/chungmeng")
## load training data
house_train <- read.csv("../data/train.csv",
header = TRUE,
na.strings = "",
stringsAsFactors = FALSE)
## load test data
house_test <- read.csv("../data/test.csv",
header = TRUE,
na.strings = "",
stringsAsFactors = FALSE)
dim(house_train)
dim(house_test)
str(house_train)
## get levels of categorical features from data description
factorLevel <- list()
conn <- file("../data/data_description.txt", open="r")
f <-readLines(conn)
for (line in f){
if(!grepl("^[[:blank:]]", line) & grepl(": ", line)) {
col_name <<- trimws(gsub(":.*", "", line))
} else {
level <- trimws(gsub("\t.*", "", line))
if (level != "") {
factorLevel[[col_name]] <- c(factorLevel[[col_name]], level)
}
}
}
close(conn)
print(factorLevel[1:6])
## check if levels in description cover unique data values
for (varname in names(factorLevel)) {
levelDiff <- setdiff(unique(house_train[[varname]]),
factorLevel[[varname]])
if(length(levelDiff)) {
print(paste(varname,
paste(levelDiff, collapse = ", "),
sep = ": "))
}
}
## fix those levels that don't match with data
## ignore "NA" as they will be considered as missing when converting categorical to factors
unique(house_train$MSZoning)
factorLevel$MSZoning
factorLevel$MSZoning[2] <- "C (all)"
unique(house_train$Neighborhood)
factorLevel$Neighborhood
factorLevel$Neighborhood[13] <- "NAmes"
unique(house_train$BldgType)
factorLevel$BldgType
factorLevel$BldgType[c(2,3,5)] <- c("2fmCon","Duplex","Twnhs")
unique(house_train$Exterior2nd)
factorLevel$Exterior2nd
factorLevel$Exterior2nd[c(17,6,3)] <- c("Wd Shng","CmentBd","Brk Cmn")
## Get levels that only appear in the dataset
for (varname in names(factorLevel)) {
factorLevel[[varname]] <- intersect(factorLevel[[varname]],
unique(house_train[[varname]]))
}
## Re-run the previous cell to double check
## convert column datatype to numeric / factor
## On training dataset
for (varname in names(house_train)[-1]) {
if (varname %in% names(factorLevel)) {
house_train[[varname]] <- factor(house_train[[varname]],
levels = factorLevel[[varname]])
} else {
house_train[[varname]] <- as.numeric(house_train[[varname]])
}
}
## On testing dataset
for (varname in names(house_test)[-1]) {
if (varname %in% names(factorLevel)) {
house_test[[varname]] <- factor(house_test[[varname]],
levels = factorLevel[[varname]])
} else {
house_test[[varname]] <- as.numeric(house_test[[varname]])
}
}
house_train$Id <- NULL
rownames(house_test) <- house_test$Id
house_test$Id <- NULL
save(house_train, house_test, file = "./house_loaded.RData")
library(ggplot2)
library(gridExtra)
library(tabplot)
library(lsr)
library(corrplot)
library(dplyr)
rm(list = ls())
load("./house_loaded.RData")
## histogram on SalePrice
grid.arrange(ggplot(house_train) +
geom_histogram(aes(SalePrice), bins = 20),
ggplot(house_train) +
geom_histogram(aes(log(SalePrice + 1)), bins = 20),
ncol = 2)
## table plot all features on sortded SalePrice
colMtx <- matrix(names(house_train)[1:length(house_train)-1], nrow = 8)
for (i in 1:ncol(colMtx)) {
tableplot(house_train,
select_string = c(colMtx[,i], "SalePrice"),
sortCol = "SalePrice", decreasing = TRUE,
nBins = 30)
}
numeric_features <- names(house_train)[sapply(house_train, is.numeric)]
numeric_features <- numeric_features[-length(numeric_features)]
print(numeric_features)
## correlation between continuous variables in training dataset - pearson
corr_numtran <- cor(house_train %>%
select(one_of(numeric_features, "SalePrice")),
method = "pearson",
use = "pairwise.complete.obs")
corrplot(corr_numtran, method = "color", order="hclust")
## correlation between continuous variables in test dataset - pearson
# corr_numtest <- cor(house_test %>%
#                       select(one_of(numeric_features)),
#                     method = "pearson",
#                     use = "pairwise.complete.obs")
#
# corrplot(corr_numtest, method = "color", order="hclust")
## ordinal features are those who contain one of the follow levels
ordinal_levels <- c("Reg", "5", "TA", "No", "Unf",
"MnPrv", "Y", "Mod", "HLS", "1Fam")
ordinal_features <- character(0)
for(feature in names(house_train)) {
if(is.factor(house_train[,feature]) &&
length(intersect(ordinal_levels, levels(house_train[,feature])))) {
ordinal_features <- c(ordinal_features, feature)
}
}
print(ordinal_features)
## correlation between ordinal variables in training dataset - kendall
corr_ordtran <- cor(data.matrix(house_train %>%
select(one_of(ordinal_features))),
method = "kendall",
use = "pairwise.complete.obs")
corrplot(corr_ordtran, method = "color", order="hclust")
## correlation between ordinal variables in test dataset - kendall
# corr_ordtest <- cor(data.matrix(house_test %>%
#                                   select(one_of(ordinal_features))),
#                     method = "kendall",
#                     use = "pairwise.complete.obs")
#
# corrplot(corr_ordtest, method = "color", order="hclust")
## CramÃ©r's V is a measure of association between two nominal variables, giving a value between 0 and +1 (inclusive)
cor.cramersV <- function(data) {
cramersV(table(data[complete.cases(data),]))
}
nominal_features <- setdiff(names(house_train),
c(numeric_features, ordinal_features, "SalePrice"))
## cramers V in test dataset
corr_nomtran <- sapply(nominal_features,
function(x) sapply(nominal_features,
function(y) cor.cramersV(house_train[, c(x, y)])))
corrplot(corr_nomtran, method = "color", order="hclust")
## coorelation between ordered categorical variables in training - spearman
cor.ordcnt <- function(data, x, y) {
cor(as.numeric(data[[x]]), as.numeric(data[[y]]),
method = "spearman",
use = "pairwise.complete.obs")
}
corr_ordcnttran <- data.frame(Variable = ordinal_features,
Correlation = sapply(ordinal_features,
function(x) -cor.ordcnt(house_train, x, "SalePrice")))
ggplot(corr_ordcnttran, aes(reorder(Variable, Correlation), Correlation)) +
geom_bar(stat = "identity") +
coord_flip()
## Might be a good idea to convert some ordinal predictors to continuous
grid.arrange(
ggplot(house_train, aes(x = OverallQual, y = SalePrice)) + geom_boxplot(),
ggplot(house_train, aes(x = ExterQual, y = SalePrice)) + geom_boxplot(),
ggplot(house_train, aes(x = BsmtQual, y = SalePrice)) + geom_boxplot(),
ggplot(house_train, aes(x = KitchenQual, y = SalePrice)) + geom_boxplot(),
ggplot(house_train, aes(x = GarageFinish, y = SalePrice)) + geom_boxplot(),
ggplot(house_train, aes(x = FireplaceQu, y = SalePrice)) + geom_boxplot(),
ncol = 2
)
grid.arrange(
ggplot(house_train, aes(x = as.integer(OverallQual), y = SalePrice)) + geom_point(),
ggplot(house_train, aes(x = as.integer(ExterQual), y = SalePrice)) + geom_point(),
ggplot(house_train, aes(x = as.integer(BsmtQual), y = SalePrice)) + geom_point(),
ggplot(house_train, aes(x = as.integer(KitchenQual), y = SalePrice)) + geom_point(),
ggplot(house_train, aes(x = as.integer(GarageFinish), y = SalePrice)) + geom_point(),
ggplot(house_train, aes(x = as.integer(FireplaceQu), y = SalePrice)) + geom_point(),
ncol = 2
)
tableplot(house_train %>% select(one_of("SalePrice","OverallQual", "ExterQual", "BsmtQual",
"KitchenQual", "GarageFinish", "FireplaceQu")),
decreasing = TRUE,
nBins = 18,
colorNA = "#FF1414", colorNA_num = "#FF1414")
library(VIM)
library(caret)
library(RANN)
## check missing values
col_missing <- names(house_train)[colSums(is.na(house_train)) > 0]
aggr(house_train[,col_missing], prop = F, numbers = T)
Filter(function(x) x > 0, colSums(is.na(house_train)))
col_missing <- names(house_test)[colSums(is.na(house_test)) > 0]
aggr(house_test[,col_missing], prop = F, numbers = T)
Filter(function(x) x > 0, colSums(is.na(house_test)))
## table plot all features on sortded SalePrice
tableplot(house_train %>% select(starts_with("Garage")),
decreasing = TRUE,
nBins = 18,
colorNA = "#FF1414", colorNA_num = "#FF1414")
tableplot(house_test %>% select(starts_with("Garage")),
decreasing = TRUE,
nBins = 19,
colorNA = "#FF1414", colorNA_num = "#FF1414")
grid.arrange(
ggplot(house_train,
aes(YearBuilt, ifelse(is.na(GarageYrBlt), YearBuilt, GarageYrBlt))) +
geom_point(aes(color = is.na(GarageYrBlt))),
ggplot(house_test,
aes(YearBuilt, ifelse(is.na(GarageYrBlt), YearBuilt, GarageYrBlt))) +
geom_point(aes(color = is.na(GarageYrBlt))),
ncol = 2
)
## Fix outlier
house_test$GarageYrBlt[which(house_test$GarageYrBlt == 2207)] <- 2007
## Create new feature: hasGarage
## Impute GarageYrBlt with YearBuilt
house_train <- house_train %>%
mutate(hasGarage = ifelse(is.na(GarageYrBlt), 0, 1),
GarageBlt = ifelse(is.na(GarageYrBlt), 0, GarageYrBlt - YearBuilt)) %>%
select(-GarageYrBlt)
house_test <- house_test %>%
mutate(hasGarage = ifelse(is.na(GarageYrBlt), 0, 1),
GarageBlt = ifelse(is.na(GarageYrBlt), 0, GarageYrBlt - YearBuilt)) %>%
select(-GarageYrBlt)
grid.arrange(
ggplot(house_train, aes(YearBuilt, GarageBlt)) +
geom_point(aes(color = as.factor(hasGarage))),
ggplot(house_test, aes(YearBuilt, GarageBlt)) +
geom_point(aes(color = as.factor(hasGarage))),
ncol = 2
)
grid.arrange(
ggplot(house_train, aes(YearBuilt, YearRemodAdd)) +
geom_point(aes(color = (YearRemodAdd == YearBuilt))),
ggplot(house_test, aes(YearBuilt, YearRemodAdd)) +
geom_point(aes(color = (YearRemodAdd == YearBuilt))),
ncol = 2
)
house_train <- house_train %>%
mutate(isRemod = ifelse(YearRemodAdd == YearBuilt, 0, 1),
RemodAdd = YearRemodAdd - YearBuilt) %>%
select(-YearRemodAdd)
house_test <- house_test %>%
mutate(isRemod = ifelse(YearRemodAdd == YearBuilt, 0, 1),
RemodAdd = YearRemodAdd - YearBuilt) %>%
select(-YearRemodAdd)
grid.arrange(
ggplot(house_train, aes(YearBuilt,RemodAdd)) +
geom_point(aes(color = as.factor(isRemod))),
ggplot(house_test, aes(YearBuilt, RemodAdd)) +
geom_point(aes(color = as.factor(isRemod))),
ncol = 2
)
nzv <- nearZeroVar(house_train[, -length(house_train)],
freqCut = 99/1,
uniqueCut = 5,
saveMetrics= TRUE)
nzv[nzv$nzv,]
house_train[, rownames(nzv[nzv$nzv,])] <- NULL
house_test[, rownames(nzv[nzv$nzv,])] <- NULL
dim(house_train)
dim(house_test)
## use knn to impute all numerical varialbes
# preProc_numerical <- preProcess(house_train[, -length(house_train)],
#                                 method = c("knnImpute"))
# print(preProc_numerical)
# house_train[, -length(house_train)] <- predict(preProc_numerical,
#                               house_train[, -length(house_train)])
# house_test <- predict(preProc_numerical, house_test)
## Caret only handles numerical features
## use kNN to impute categorical features
house_train[, -length(house_train)] <- kNN(house_train[, -length(house_train)],
k=5)[,names(house_train)[-length(house_train)]]
house_test <- kNN(house_test, k=5)[,names(house_test)]
## Double check missingness with the following code
aggr(house_train, prop = F, numbers = T)
aggr(house_test, prop = F, numbers = T)
## Transform SalePrice to log scale
house_train$SalePrice <- log(house_train$SalePrice + 1)
save(house_train, house_test, file = "./house_preProc.RData")
rm(list = ls()) # clear workspace
library(caret)
load("./house_preProc.RData")
## Perform single 80%/20% random split of house_train
library(caret)
set.seed(321)
trainIdx <- createDataPartition(house_train$SalePrice,
p = .8,
list = FALSE,
times = 1)
subTrain <- house_train[trainIdx,]
subTest <- house_train[-trainIdx,]
print(head(subTrain))
set.seed(456)
fitCtrl <- trainControl(method = "repeatedcv",
number = 5,
repeats = 3,
verboseIter = FALSE,
summaryFunction = defaultSummary)
lmFit <- train(SalePrice ~., data = subTrain,
method = "lm")
# summary(lmFit)
## Call:
## lm(formula = .outcome ~ ., data = dat)
## ... ...
## Residual standard error: 0.1152 on 915 degrees of freedom
## Multiple R-squared:  0.935,	Adjusted R-squared:  0.917
## F-statistic:    52 on 253 and 915 DF,  p-value: < 2.2e-16
lmImp <- varImp(lmFit, scale = FALSE)
## lm variable importance
##
##  only 20 most important variables shown (out of 253)
##
##                      Overall
## MSZoningRL             8.151
## MSZoningFV             7.881
## MSZoningRM             7.659
## MSZoningRH             6.977
## SaleConditionAbnorml   5.433
## OverallQual1           5.309
## LandContourBnk         4.292
plot(lmImp, top = 20)
mean(lmFit$resample$RMSE)
predicted <- predict(lmFit, subTest)
RMSE(pred = predicted, obs = subTest$SalePrice)
ggplot(subTest, aes(x = exp(SalePrice)-1, y = exp(predicted)-1)) +
geom_point() +
coord_fixed()
enetGrid <- expand.grid(alpha = seq(0, 1, .1),
lambda = seq(0, .6, .01))
set.seed(1234)  # for reproducibility
enetFit <- train(SalePrice ~ .,
data = subTrain,
method="glmnet",
metric="RMSE",
trControl=fitCtrl,
tuneGrid=enetGrid)
print(enetFit$bestTune)
plot(enetFit)
plot(enetFit, plotType = "level")
enetVarImp <- varImp(enetFit, scale = FALSE)
plot(enetVarImp, top = 20)
mean(enetFit$resample$RMSE)
mean(enetFit$resample$RMSE)
predicted <- predict(enetFit, subTest)
RMSE(pred = predicted, obs = subTest$SalePrice)
subTest$predicted <- predict(enetFit, subTest)
ggplot(subTest, aes(x = SalePrice, y = predicted)) + geom_point()
fitCtrl <- trainControl(method = "cv",
number = 5,
verboseIter = TRUE,
summaryFunction=defaultSummary)
gbmGrid <- expand.grid( n.trees = seq(100,1000,100),
interaction.depth = seq(1,10,2),
shrinkage = c(0.1),
n.minobsinnode = 10)
gbmFit <- train(SalePrice ~ .,
data = subTrain,
method = "gbm",
trControl = fitCtrl,
tuneGrid=gbmGrid,
metric='RMSE',
maximize=FALSE)
plot(gbmFit)
gbmFit$bestTune
mean(gbmFit$resample$RMSE)
predicted <- predict(gbmFit, subTest)
RMSE(pred = predicted, obs = subTest$SalePrice)
ggplot(subTest, aes(x = exp(SalePrice)-1, y = exp(predicted)-1)) +
geom_point() +
coord_fixed()
names(house_train)
dim(house_train)
str(house_train)
names(house_train)
library(VIM)
aggr(house_train)
library(mice)
md.pattern(house_train)
rm(list=ls())
setwd("C:/Users/cmlim/Dropbox/NYC Data Science/Class/Machine Learning Project/chungmeng")
house_train <- read.csv("../data/train.csv",
header = TRUE,
na.strings = "",
stringsAsFactors = FALSE)
## load test data
# house_test <- read.csv("../data/test.csv",
#                        header = TRUE,
#                        na.strings = "",
#                        stringsAsFactors = FALSE)
#dim(house_test)
dim(house_train)
str(house_train)
names(house_train)
library(VIM)
aggr(house_train)
library(mice)
md.pattern(house_train)
## get levels of categorical features from data description
factorLevel <- list()
conn <- file("../data/data_description.txt", open="r")
f <-readLines(conn)
for (line in f){
if(!grepl("^[[:blank:]]", line) & grepl(": ", line)) {
col_name <<- trimws(gsub(":.*", "", line))
} else {
level <- trimws(gsub("\t.*", "", line))
if (level != "") {
factorLevel[[col_name]] <- c(factorLevel[[col_name]], level)
}
}
}
close(conn)
print(factorLevel[1:6])
print(factorLevel[1:20])
