install.packages('e1071')
set.seed(0)
x1 = c(rnorm(100, 0, 4), rnorm(100, 1, 3))
x2 = c(rnorm(100, 0, 1), rnorm(100, 6, 1))
y = as.factor(c(rep(-1, 100), rep(1, 100)))
linearly.separable = data.frame(x1, x2, y)
plot(linearly.separable$x1, linearly.separable$x2, col = linearly.separable$y)
set.seed(0)
train.index = sample(1:200, 200*.8)
test.index = -train.index
library(e1071)
#Fitting a maximal margin classifier to the training data.
svm.mmc.linear = svm(y ~ ., #Familiar model fitting notation.
data = linearly.separable, #Using the linearly separable data.
subset = train.index, #Using the training data.
kernel = "linear", #Using a linear kernel.
cost = 1e6) #A very large cost; default is 1.
plot(svm.mmc.linear, linearly.separable[train.index, ])
svm.mmc.linear
svm.mmc.linear$index
summary(svm.mmc.linear)
ypred = predict(svm.mmc.linear, linearly.separable[test.index, ])
table("Predicted Values" = ypred, "True Values" = linearly.separable[test.index, "y"])
#Adding a single point to display the sensitivity of the maximal margin classifier.
linearly.separable2 = rbind(linearly.separable, c(-5, 3, 1))
plot(linearly.separable2$x1, linearly.separable2$x2, col = linearly.separable2$y)
#Fitting a maximal margin classifier to the new data.
svm.mmc.linear2 = svm(y ~ .,
data = linearly.separable2,
kernel = "linear",
cost = 1e6)
plot(svm.mmc.linear, linearly.separable[train.index, ]) #Old model.
plot(svm.mmc.linear2, linearly.separable2) #New model.
table("Predicted Values" = ypred, "True Values" = linearly.separable[test.index, "y"])
svm.mmc.linear2 = svm(y ~ .,
data = linearly.separable2,
kernel = "linear",
cost = 1e6)
#Visualizing the results of the maximal margin classifier; comparing the output.
plot(svm.mmc.linear, linearly.separable[train.index, ]) #Old model.
plot(svm.mmc.linear2, linearly.separable2) #New model.
#Additional information for the fit.
summary(svm.mmc.linear2)
svm.mmc.linear2$index
svm.mmc.linear2 = svm(y ~ .,
data = linearly.separable2,
kernel = "linear",
cost = 1)
#Visualizing the results of the maximal margin classifier; comparing the output.
plot(svm.mmc.linear, linearly.separable[train.index, ]) #Old model.
plot(svm.mmc.linear2, linearly.separable2) #New model.
#Additional information for the fit.
summary(svm.mmc.linear2)
#Finding the indices of the support vectors.
svm.mmc.linear2$index
#Fitting a maximal margin classifier to the new data.
svm.mmc.linear2 = svm(y ~ .,
data = linearly.separable2,
kernel = "linear",
cost = 1e6)
#Visualizing the results of the maximal margin classifier; comparing the output.
plot(svm.mmc.linear, linearly.separable[train.index, ]) #Old model.
plot(svm.mmc.linear2, linearly.separable2) #New model.
#Additional information for the fit.
summary(svm.mmc.linear2)
#Finding the indices of the support vectors.
svm.mmc.linear2$index
#Fitting a support vector classifier by reducing the cost of a misclassified
#observation.
svm.svc.linear2 = svm(y ~ .,
data = linearly.separable2,
kernel = "linear",
cost = 1)
#Visualizing the results of the support vector classifier.
plot(svm.svc.linear2, linearly.separable2)
summary(svm.svc.linear2)
svm.svc.linear2$index
#What happens if we reduce the cost even more?
svm.svc.linear3 = svm(y ~ .,
data = linearly.separable2,
kernel = "linear",
cost = .1)
plot(svm.svc.linear3, linearly.separable2)
summary(svm.svc.linear3)
svm.svc.linear3$index
svm.svc.linear3
class(svm.svc.linear3)
set.seed(0)
x1 = c(rnorm(100, -1, 1), rnorm(100, 1, 1))
x2 = c(rnorm(100, -1, 1), rnorm(100, 1, 1))
y = as.factor(c(rep(-1, 100), rep(1, 100)))
overlapping = data.frame(x1, x2, y)
plot(overlapping$x1, overlapping$x2, col = overlapping$y)
?rnorm
factorLevel <- list()
conn <- file("./data_description.txt", open="r")
setwd("C:/Users/cmlim/Dropbox/NYC Data Science/Class/Machine Learning Project/Wing")
factorLevel <- list()
conn <- file("./data_description.txt", open="r")
factorLevel <- list()
conn <- file("../data/data_description.txt", open="r")
f <-readLines(conn)
for (line in f){
if(!grepl("^[[:blank:]]", line) & grepl(": ", line)) {
col_name <<- trimws(gsub(":.*", "", line))
} else {
level <- trimws(gsub("\t.*", "", line))
if (level != "") {
factorLevel[[col_name]] <- c(factorLevel[[col_name]], level)
}
}
}
close(conn)
print(factorLevel[1:6])
factorLevel
library(data.table)
library(ggplot2)
library(gridExtra)
library(tabplot)
library(lsr)
library(corrplot)
library(dplyr)
library(VIM)
library(caret)
library(RANN)
library(reshape2)
getwd()
house_train <- read.csv("./train.csv",
header = TRUE,
na.strings = c("", "NA"),
stringsAsFactors = FALSE)
setwd("C:/Users/cmlim/Dropbox/NYC Data Science/Class/Machine Learning Project/Wing")
library(data.table)
library(ggplot2)
library(gridExtra)
library(tabplot)
library(lsr)
library(corrplot)
library(dplyr)
library(VIM)
library(caret)
library(RANN)
library(reshape2)
getwd()
house_train <- read.csv("./train.csv",
header = TRUE,
na.strings = c("", "NA"),
stringsAsFactors = FALSE)
